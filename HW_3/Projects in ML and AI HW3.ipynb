{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1. \n",
    "https://www.kaggle.com/c/digit-recognizer/data\n",
    "\n",
    "For this problem most of the datasets I have seen being used in order to train neural networks has been drawn digits \n",
    "with rows and columns of datasets containing the value of darkness to represent whether the pixel location is basically \n",
    "a white space or part of the digit creation. If figured that this seemed like a simple enough dataset to use and because \n",
    "of all the different examples I saw that means I will also have resources to use if I ever become stuck on a concept.\n",
    "\n",
    "\n",
    "Every list that seems to talk about what are the best options for frameworks to create a deep learning model all include \n",
    "TensorFlow. Not to mention TensorFlow also has its own implementation for keras, another popular framework for deep \n",
    "learning potentially giving me two for the price of one in frameworks (https://www.tensorflow.org/api_docs/python/tf/keras).\n",
    "Thanks to TensorFlow’s use in research and production purposes, it also has its own form of optimization techniques \n",
    "allowing for it to perform complicated mathematical operations like the ones that will be used in the neural net which \n",
    "can then be used to run the model on even simple devices like my phone. \n",
    "(https://www.tutorialspoint.com/what-is-tensorflow-and-how-keras-work-with-tensorflow-to-create-neural-networks) \n",
    "(https://www.analyticsvidhya.com/blog/2016/10/an-introduction-to-implementing-neural-networks-using-tensorflow/). \n",
    "Adding on to this, TensorFlow also appears to have a dedicated set of functions used in order to work with neural \n",
    "networks. Including a function for every kind of activation function that can be used in the neural network layers which \n",
    "will make performing any of the hard math components that much easier to follow \n",
    "(https://www.tensorflow.org/api_docs/python/tf/nn). \n",
    "\n",
    "\n",
    "https://www.askpython.com/python/python-frameworks-for-deep-learning\n",
    "https://thecleverprogrammer.com/2021/05/16/deep-learning-frameworks-in-python/\n",
    "https://techvidvan.com/tutorials/python-deep-learning-libraries/\n",
    "https://data-flair.training/blogs/deep-learning-with-python-libraries/\n",
    "https://www.kdnuggets.com/2017/02/python-deep-learning-frameworks-overview.html\n",
    "https://hprc.tamu.edu/files/training/2021/Spring/Introduction_to_DL_with_TensorFlow.pdf\n",
    "https://towardsdatascience.com/building-your-first-neural-network-in-tensorflow-2-tensorflow-for-hackers-part-i-e1e2f1dfe7a0\n",
    "https://www.bmc.com/blogs/create-neural-network-with-tensorflow/\n",
    "https://adventuresinmachinelearning.com/python-tensorflow-tutorial/\n",
    "https://www.analyticsvidhya.com/blog/2016/10/an-introduction-to-implementing-neural-networks-using-tensorflow/\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('digit_train.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.use_inf_as_na = True\n",
    "data.fillna(0)\n",
    "data.columns[data.isnull().any()]\n",
    "data = data.astype(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now declare the weights connecting the input to the hidden layer \n",
    "W1 = tf.Variable(tf.random.normal([784, 100], stddev=0.01), name='W1')\n",
    "b1 = tf.Variable(tf.random.normal([100]), name='b1')\n",
    "# and the weights connecting the hidden layer to the output layer\n",
    "W2 = tf.Variable(tf.random.normal([100, 10], stddev=0.01), name='W2')\n",
    "b2 = tf.Variable(tf.random.normal([10]), name='b2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Z = Wx + b)\n",
    "### https://adventuresinmachinelearning.com/vanishing-gradient-problem-tensorflow/\n",
    "'''\n",
    "Here is were the feed forward algorithm is implemented. For the activation function I went with Leaky Relu because\n",
    "the Relu function is believed to be the best function to use in deep learning, and the leaky relu is basically the same\n",
    "as Relu only it fixes any of the inconveniences that can happen if x < 0. \n",
    "'''\n",
    "def feed_forward(x_input, W1, b1, W2, b2):\n",
    "    W = tf.reshape(x_input, (x_input.shape[0], -1))\n",
    "    W = tf.cast(W, tf.float32)\n",
    "    \n",
    "    W = tf.reshape(W,[-1,784])\n",
    "    \n",
    "    x = tf.add(tf.matmul(W, W1), b1)\n",
    "    x = tf.nn.leaky_relu(x)\n",
    "    output = tf.add(tf.matmul(x, W2), b2)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, loss=0.75415, test set      accuracy=87.89809%\n",
      "Epoch: 2, loss=0.12285, test set      accuracy=87.89809%\n",
      "Epoch: 3, loss=0.10657, test set      accuracy=87.89809%\n",
      "Epoch: 4, loss=0.07347, test set      accuracy=87.89809%\n",
      "Epoch: 5, loss=0.09412, test set      accuracy=87.89809%\n",
      "Epoch: 6, loss=0.08885, test set      accuracy=87.89809%\n",
      "Epoch: 7, loss=0.07805, test set      accuracy=87.89809%\n",
      "Epoch: 8, loss=0.07169, test set      accuracy=87.89809%\n",
      "Epoch: 9, loss=0.07252, test set      accuracy=87.89809%\n",
      "Epoch: 10, loss=0.05899, test set      accuracy=87.89809%\n",
      "\n",
      "Training complete!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://www.youtube.com/watch?v=w8yWXqWQYmU\n",
    "# https://towardsdatascience.com/building-your-first-neural-network-in-tensorflow-2-tensorflow-for-hackers-part-i-e1e2f1dfe7a0\n",
    "# https://adventuresinmachinelearning.com/python-tensorflow-tutorial/\n",
    "# https://www.analyticsvidhya.com/blog/2016/10/an-introduction-to-implementing-neural-networks-using-tensorflow/\n",
    "\n",
    "data = np.array(data)\n",
    "m, n = data.shape\n",
    "np.random.shuffle(data)\n",
    "\n",
    "data_dev = data[0:1000]\n",
    "Y_dev = data_dev[0]\n",
    "X_dev = data_dev[1:n]\n",
    "X_dev = np.reshape(X_dev,(n,28,28))\n",
    "\n",
    "data_train = data[1000:m]\n",
    "Y_train = data_train[0]\n",
    "X_train = data_train[1:n]\n",
    "X_train = np.reshape(X_train,(n,28,28))\n",
    "\n",
    "\n",
    "data_test = pd.read_csv('digit_test.csv')\n",
    "data_test.fillna(0)\n",
    "data_test.columns[data_test.isnull().any()]\n",
    "data_test = data.astype(\"float\")\n",
    "data_test = np.array(data_test)\n",
    "m,n = data_test.shape\n",
    "\n",
    "#data_test = data_test.T\n",
    "Y_test = data_test[0]\n",
    "X_test = data_test[1:n]\n",
    "X_test = np.reshape(X_test,(n,28,28))\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Once I have separated the data into dev, train, and test sets I then take the X values of all these datasets and divide\n",
    "them by 255 which is the maximum number we can get from these datasets. This is to transform the data into sets that \n",
    "will be scaled between 0 and 1 which a lot of examples I saw do in order to make better models.\n",
    "'''\n",
    "\n",
    "X_dev = X_dev / 255.\n",
    "X_train = X_train / 255.\n",
    "X_test = X_test / 255.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
    "'''\n",
    "Thanks to the packages offered in tensorflow there were even functions that can be used to make the optimization\n",
    "algorithms, including arguments that can be filled with any added information like the learning rate. Out of the ones\n",
    "from this page the RMSprop algorithm with the hyperparameter momentum at 0.2 seems to perform the best in terms of accuracy\n",
    "score, but the Adam and Adamax algortihms were also pretty successful and with both of them having a somewhat easier time\n",
    "with having consistently decreasing loss values and mostly consistently increasing accuracy scores.\n",
    "'''\n",
    "# optimizer = tf.keras.optimizers.Adamax(learning_rate=0.001)\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate = 0.001, momentum=0.2)\n",
    "\n",
    "loss_list = []\n",
    "accuracy_list = []\n",
    "iterations = 10\n",
    "total_batch = int((len(Y_train)) / 100)\n",
    "for epoch in range(iterations):\n",
    "    avg_loss = 0\n",
    "    for i in range(total_batch):\n",
    "        '''\n",
    "        I originally tried to get the batch data from a 2D array but that made the accuracy of the model be too low\n",
    "        while also taking a longer time to compute. So instead I tried converting the data into a 3D array like\n",
    "        how I saw in most tutorials done for neural nets and that made the data easier to parse for the run time \n",
    "        while also increasing the accuracy drastically.\n",
    "        \n",
    "        One of the hyperparameters I was testing out is the mini-batch size that is taken from the training data \n",
    "        during each batch iteration. From small to large sizes each had their own various results. After\n",
    "        some comparisons using a mini-batch size of 200 seems to yield the best results so far.\n",
    "        '''\n",
    "        index = np.random.randint(0, len(Y_train), 200)\n",
    "        batch_x = X_train[index,:,:]\n",
    "        batch_y = Y_train[index]\n",
    "        \n",
    "        '''\n",
    "        I now convert all of the batch X and Y sets into tensor variables so that I may be able to use the associated \n",
    "        functions for them, such as the next one I will be using on the batch Y data with tf.one_hot. This will convert all of \n",
    "        the values in the batch Y matrix into an array of zeros with only one non-zero value in the array being the value \n",
    "        '1' which will be located in the index of the array that the current Y value is representing. To calrify, say that \n",
    "        in the array there is a value of 3, with one-hot encoding that value now becomes the array [0,0,0,1,0,0,0,0,0,0]. \n",
    "        This is mainly because all of the digits trying to be recognized are values 0-9 and the encoding array will be \n",
    "        used with the output layer as a way of classifying the answer\n",
    "        '''\n",
    "        # create tensors\n",
    "        batch_x = tf.Variable(batch_x)\n",
    "        batch_y = tf.Variable(batch_y)\n",
    "        # create a one hot vector\n",
    "        batch_y = tf.cast(batch_y,tf.int32)\n",
    "        batch_y = tf.one_hot(batch_y, depth = 10)\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        This tf.GradientTape() function was a big help as it's able to calculate the derivative without having to write \n",
    "        the formulas and from it aquire the gradients in order to then update the weights and biases.\n",
    "        I also use this tensorflow function to compute the loss value called softmax_cross_entropy_with_logits. It\n",
    "        takes in the labels (in this case the one_hot encoded y values) and the output values from the feed forward \n",
    "        layer and performs the softmax activation function and the cross-entropy loss function all in one function.\n",
    "        There are also other tensorflow functions that can do the same thing but after testing them the softmax with\n",
    "        cross-entropy performs the best with no errors. And the reduce_mean function is used in order to get the\n",
    "        mean values from all the tensor values that comes from the softmax cross-entropy function\n",
    "        \n",
    "        '''\n",
    "        # https://www.geeksforgeeks.org/python-tensorflow-gradienttape/\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = feed_forward(batch_x, W1, b1, W2, b2)\n",
    "            activation_loss = tf.nn.softmax_cross_entropy_with_logits(labels=batch_y,logits=logits)\n",
    "            #print(activation_loss)\n",
    "            loss = tf.reduce_mean(activation_loss)\n",
    "            #loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=batch_y,logits=logits))\n",
    "            \n",
    "            \n",
    "            # https://www.geeksforgeeks.org/python-tensorflow-gradienttape-gradient/\n",
    "            gradients = tape.gradient(loss, [W1, b1, W2, b2])\n",
    "            optimizer.apply_gradients(zip(gradients, [W1, b1, W2, b2]))\n",
    "            avg_loss += loss / total_batch\n",
    "        \n",
    "    test_logits = feed_forward(X_dev, W1, b1, W2, b2)\n",
    "    # print(test_logits)\n",
    "    max_idxs = tf.argmax(test_logits, axis=1)\n",
    "    test_acc = np.sum(max_idxs.numpy() == Y_dev) / len(Y_dev)\n",
    "    loss_list.append(avg_loss)\n",
    "    accuracy_list.append(test_acc)\n",
    "    print(f\"Epoch: {epoch+1}, loss={avg_loss:.5f}, test set      accuracy={test_acc*100:.5f}%\")\n",
    "print(\"\\nTraining complete!\")\n",
    "\n",
    "it = [i for i in range(iterations)]\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(it,loss_list)\n",
    "plt.show()\n",
    "\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(it,accuracy_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/40879504/how-to-apply-drop-out-in-tensorflow-to-improve-the-accuracy-of-neural-network\n",
    "# https://datascience.stackexchange.com/questions/31248/how-to-improve-accuracy-of-deep-neural-networks\n",
    "# https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques/\n",
    "# https://deeplizard.com/learn/video/0h8lAm5Ki5g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy=87.89809%\n"
     ]
    }
   ],
   "source": [
    "test_logits = feed_forward(X_test, W1, b1, W2, b2)\n",
    "max_idxs = tf.argmax(test_logits, axis=1)\n",
    "test_acc = np.sum(max_idxs.numpy() == Y_test) / len(Y_test)\n",
    "print(f\"accuracy={test_acc*100:.5f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "3. \n",
    "When it came to choosing the hyperparameters for this HW I mostly did a lot of trial and error testing and\n",
    "compared results to get what I have now. I at first just started with the blank tensorflow functions that I have now\n",
    "using just their default argument values to see how they would work at first. Once I felt the process was complete I then \n",
    "started playing around with the argument values of the functions to see if I could get better results. My version of \n",
    "getting better results usually just consisted of having a continually decreasing cost value as well as a good accuracy \n",
    "score. I figured this was the best way of measuring the competency of the methods. Based on what each of the functions\n",
    "I have used do in their document explanations, it appears that I did not use regularization and felt no need to use it\n",
    "during the training. The results I were getting were mostly in the 80-90 accuracy range and some even in the 70-80 range\n",
    "this meant the my implementation was no longer underfitting like it was before I changed the shape of the data into\n",
    "3D arrays and it is most likely not overfitting due to it not constantly being in the 90-100 range. Therefore, I felt no\n",
    "reason to try and use regularization. I did use and test out different optimization algorithms that tensorflow had to offer.\n",
    "Unlike with regularization, which is mostly used in order to fix overfitting issues, the optimization algorithm is needed\n",
    "to find the best set of data inputs that can train the model to achieve better results. Making it practically a \n",
    "necessity to add to the ML process.\n",
    "\n",
    "Regardless, the decision of using the tensorflow framework for this was a very big help. In just one line of code\n",
    "I was able to get results that would have taken me countless trial and error of coding and debugging to try\n",
    "and make sure that I make a clear and operationable function. \n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
