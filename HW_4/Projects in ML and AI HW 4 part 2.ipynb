{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\petert6\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\petert6\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\petert6\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\petert6\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\petert6\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\petert6\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import re\n",
    "import string\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#!pip install gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://rare-technologies.com/word2vec-tutorial/\n",
    "# https://radimrehurek.com/gensim/models/word2vec.html\n",
    "# https://www.educative.io/edpresso/how-to-find-similarity-between-two-words-using-nlp\n",
    "# https://openclassrooms.com/en/courses/6532301-introduction-to-natural-language-processing/7132231-train-your-first-embedding-models\n",
    "\n",
    "'''\n",
    "Unlike with other implementations of Word2Vec I have seen, this one I am using is basically a blank slate. So, in order to \n",
    "use the Word2Vec model I first need to train the model with a series of words/sentences. And since I am already using\n",
    "the shakespeare text for part 1 I figured I can use it for part 2 as well\n",
    "'''\n",
    "\n",
    "text = open('shakespeare.txt', 'rb').read().decode(encoding='utf-8')\n",
    "lines = text.split('\\n')\n",
    "sentences = []\n",
    "for line in lines:\n",
    "    # remove punctuation\n",
    "    line = re.sub(r'[\\!\"#$%&\\*+,-./:;<=>?@^_`()|~=]','',line).strip()\n",
    "    # tokenizer\n",
    "    tokens = re.findall(r'\\b\\w+\\b', line)\n",
    "    if len(tokens) > 1:\n",
    "        sentences.append(tokens)\n",
    "\n",
    "model = Word2Vec(sentences=sentences, min_count=3, vector_size = 50, sg = 1, window = 7)\n",
    "model.save(\"word2vec.model\")\n",
    "\n",
    "model = Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_word_embed(model):\n",
    "    word1 = input(\"Enter the first word to be compared: \")\n",
    "    if(word1 not in model.index_to_key):\n",
    "        while(word1 not in model.index_to_key):\n",
    "            print(\"I am sorry but [\"+word1+\"] is not in the list of words\")\n",
    "            word1 = input(\"Please enter another word: \")\n",
    "    word2 = input(\"Enter the second word to be compared: \")\n",
    "    if(word2 not in model.index_to_key):\n",
    "        while(word2 not in model.index_to_key):\n",
    "            print(\"I am sorry but [\"+word2+\"] is not in the list of words\")\n",
    "            word2 = input(\"Please enter another word: \")\n",
    "            \n",
    "    w1 = model[word1]\n",
    "    w2 = model[word2]\n",
    "    cosine_similarity = np.dot(w1, w2)/(np.linalg.norm(w1)* np.linalg.norm(w2))\n",
    "    PIP_loss = np.sqrt(abs(np.dot(w1,w1.T) - np.dot(w2,w2.T)))\n",
    "    \n",
    "    print()\n",
    "    print(\"The Cosine Similarity between [\"+word1+\"] and [\"+word2+\"] is:\",cosine_similarity)\n",
    "    print(\"The dissimilarity between [\"+word1+\"] and [\"+word2+\"] is:\",PIP_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the first word to be compared: fool\n",
      "Enter the second word to be compared: king\n",
      "\n",
      "The Cosine Similarity between [fool] and [king] is: 0.52015287\n",
      "The dissimilarity between [fool] and [king] is: 0.49285793\n"
     ]
    }
   ],
   "source": [
    "calculate_word_embed(model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "At first when I wanted to try and calculate the dissimilarity value I figured subtracting the cosine similarity value\n",
    "from 1 would be enough but that just felt lazy to me. Not to mention I did not have any kind of proof that this would be \n",
    "a correct method to acquiring the score. Not being one for taking a chance, I decided to follow the advice of the homework \n",
    "and started reading research papers to find out more about determining the dissimilarity between words. I then came across\n",
    "this one paper called \"On the Dimensionality of Word Embedding\"\n",
    "(https://papers.nips.cc/paper/2018/file/b534ba68236ba543ae44b22bd110a1d6-Paper.pdf). Which talked about a method called \n",
    "Pairwise Inner Product Loss (PIP Loss). This basically involved taking both the embeded word matrices, multiplying each by\n",
    "their transpose value, then subtracting from each other and getting the square root of that subtracted value. I honestly\n",
    "think this would be a good way of identifying the dissimilarity between the 2. This would be because by first multiplying\n",
    "the embedding with its transpose form, it then forms that identity matrix for that word essentialy turning it from a regular\n",
    "n-vector to a vector representing itself. This means we are now getting the full projection of word2 onto the subspace\n",
    "of word1 showing how different they can be\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
